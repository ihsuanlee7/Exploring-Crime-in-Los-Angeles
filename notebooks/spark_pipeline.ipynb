{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#original version\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Import and setup\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV ETL Pipeline\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF2SPMA78XYT",
        "outputId": "e8fd51bf-995a-43b3-a1c3-068431cfc2d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Waiting for headers] [Connected to r2u.stat.\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 1s (194 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eGZ4Om6hj_F",
        "outputId": "6002301d-73fc-4149-c360-1eb43b9ddba1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.26\" 2025-01-21\n",
            "OpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FIXME:change directory to where the csv file are stored in EC2\n",
        "crime = spark.read.csv(\"/content/drive/MyDrive/405 final/crime.csv\", header=True, inferSchema=True)\n",
        "#business = spark.read.csv(\"/content/drive/MyDrive/405 final/business.csv\", header=True, inferSchema=True)\n",
        "ins = spark.read.csv(\"/content/drive/MyDrive/405 final/ins.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "nacVpSRH-pKO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "crime cleaning:\n",
        "Crime: Premis Desc and Crm Cd 1 columns not null \\\\\n",
        "convert 'Date Rptd'into datetime, and extract Year and month \\\\\n",
        "convert dateOCC into timestamp and find DayOfWeek \\\\\n",
        "Convert Time OCC into hour by floor divide by 100 and called hour  \\\\\n",
        "Vict Sex, Vict Descent: change null to 'Unidentified'"
      ],
      "metadata": {
        "id": "SzZ7dpmC0LaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count the null value in each column\n",
        "from pyspark.sql.functions import col, count, when\n",
        "crime.select([count(when(col(c).isNull(), c)).alias(c) for c in crime.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii9Ik-wVDnDF",
        "outputId": "dfc0d74e-e5c2-4a59-831f-6e127602b741"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+--------+--------+----+---------+-----------+--------+------+-----------+-------+--------+--------+------------+---------+-----------+--------------+-----------+------+-----------+--------+--------+--------+--------+--------+------------+---+---+\n",
            "|DR_NO|Date Rptd|DATE OCC|TIME OCC|AREA|AREA NAME|Rpt Dist No|Part 1-2|Crm Cd|Crm Cd Desc|Mocodes|Vict Age|Vict Sex|Vict Descent|Premis Cd|Premis Desc|Weapon Used Cd|Weapon Desc|Status|Status Desc|Crm Cd 1|Crm Cd 2|Crm Cd 3|Crm Cd 4|LOCATION|Cross Street|LAT|LON|\n",
            "+-----+---------+--------+--------+----+---------+-----------+--------+------+-----------+-------+--------+--------+------------+---------+-----------+--------------+-----------+------+-----------+--------+--------+--------+--------+--------+------------+---+---+\n",
            "|    0|        0|       0|       0|   0|        0|          0|       0|     0|          0| 151692|       0|  144720|      144732|       16|        588|        677816|     677816|     1|          0|      11|  935908| 1002735| 1004986|       0|      850816|  0|  0|\n",
            "+-----+---------+--------+--------+----+---------+-----------+--------+------+-----------+-------+--------+--------+------------+---------+-----------+--------------+-----------+------+-----------+--------+--------+--------+--------+--------+------------+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crime.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7wkZU5DVMo",
        "outputId": "0b248fdd-7466-413f-ea0d-38b70d8859da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DR_NO: integer (nullable = true)\n",
            " |-- Date Rptd: string (nullable = true)\n",
            " |-- DATE OCC: string (nullable = true)\n",
            " |-- TIME OCC: integer (nullable = true)\n",
            " |-- AREA: integer (nullable = true)\n",
            " |-- AREA NAME: string (nullable = true)\n",
            " |-- Rpt Dist No: integer (nullable = true)\n",
            " |-- Part 1-2: integer (nullable = true)\n",
            " |-- Crm Cd: integer (nullable = true)\n",
            " |-- Crm Cd Desc: string (nullable = true)\n",
            " |-- Mocodes: string (nullable = true)\n",
            " |-- Vict Age: integer (nullable = true)\n",
            " |-- Vict Sex: string (nullable = true)\n",
            " |-- Vict Descent: string (nullable = true)\n",
            " |-- Premis Cd: integer (nullable = true)\n",
            " |-- Premis Desc: string (nullable = true)\n",
            " |-- Weapon Used Cd: integer (nullable = true)\n",
            " |-- Weapon Desc: string (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            " |-- Status Desc: string (nullable = true)\n",
            " |-- Crm Cd 1: integer (nullable = true)\n",
            " |-- Crm Cd 2: integer (nullable = true)\n",
            " |-- Crm Cd 3: integer (nullable = true)\n",
            " |-- Crm Cd 4: integer (nullable = true)\n",
            " |-- LOCATION: string (nullable = true)\n",
            " |-- Cross Street: string (nullable = true)\n",
            " |-- LAT: double (nullable = true)\n",
            " |-- LON: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dayofweek\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "crime = crime.filter(crime[\"Premis Desc\"].isNotNull() & crime[\"Crm Cd 1\"].isNotNull())\n",
        "# convert Date Rptd and Date OCC column from string to datetime\n",
        "crime = crime.withColumn(\"Date Rptd\",\n",
        "    when(\n",
        "        crime[\"Date Rptd\"].rlike(r\"\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2} [AP]M\"),\n",
        "        to_timestamp(crime[\"Date Rptd\"], \"MM/dd/yyyy hh:mm:ss a\")\n",
        "    ).otherwise(None))\n",
        "crime = crime.withColumn(\"DATE OCC\",\n",
        "    when(\n",
        "        crime[\"DATE OCC\"].rlike(r\"\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2} [AP]M\"),\n",
        "        to_timestamp(crime[\"DATE OCC\"], \"MM/dd/yyyy hh:mm:ss a\")\n",
        "    ).otherwise(None))\n",
        "# extract the day of the week info and hour info for OCC\n",
        "crime = crime.withColumn(\"DayOfWeek OCC\", dayofweek(crime[\"DATE OCC\"]))\n",
        "crime = crime.withColumn(\"TIME OCC\", (col(\"TIME OCC\") / 100).cast(\"int\"))\n",
        "crime = crime.withColumn(\n",
        "    \"DayOfWeek OCC\",\n",
        "    when(col(\"DayOfWeek OCC\") == 1, \"Sunday\")\n",
        "    .when(col(\"DayOfWeek OCC\") == 2, \"Monday\")\n",
        "    .when(col(\"DayOfWeek OCC\") == 3, \"Tuesday\")\n",
        "    .when(col(\"DayOfWeek OCC\") == 4, \"Wednesday\")\n",
        "    .when(col(\"DayOfWeek OCC\") == 5, \"Thursday\")\n",
        "    .when(col(\"DayOfWeek OCC\") == 6, \"Friday\")\n",
        "    .when(col(\"DayOfWeek OCC\") == 7, \"Saturday\")\n",
        ")\n",
        "#fill null value\n",
        "crime = crime.withColumn(\"Vict Sex\", when(crime[\"Vict Sex\"].isNull(), \"Unidentified\").otherwise(crime[\"Vict Sex\"]))\n",
        "crime = crime.withColumn(\"Vict Descent\", when(crime[\"Vict Descent\"].isNull(), \"Unidentified\").otherwise(crime[\"Vict Descent\"]))\n"
      ],
      "metadata": {
        "id": "-i1m9NNpx6Sa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPACER"
      ],
      "metadata": {
        "id": "SJgy3llEDzra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building_and_Safety_Inspections:\n",
        "Latitude/Longitude is not null \\\\\n",
        "then Extract latitude and longitude as float from Latitude/Longitude \\\\\n",
        "\n",
        "convert 'Inspection Date'into datetime, and extract Year and month \\\\\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q-rb9-OkEGfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ins.show(10, truncate = False)"
      ],
      "metadata": {
        "id": "YwoUAWKf6xt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d8ef29-ce56-4644-bca6-44b11d583eab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----------------+--------------+---------------+---------------------+-----------------+----------------------+\n",
            "|ADDRESS                  |PERMIT           |Permit Status |Inspection Date|Inspection Type      |Inspection Result|Latitude/Longitude    |\n",
            "+-------------------------+-----------------+--------------+---------------+---------------------+-----------------+----------------------+\n",
            "|10000 W SANTA MONICA BLVD|14044 10000 02293|Issued        |07/20/2016     |Rough-Ventilation    |Partial Approval |(34.06364, -118.41437)|\n",
            "|1000 S SANTA FE AVE      |15016 10000 18196|Permit Finaled|07/22/2016     |Smoke Detectors      |Insp Cancelled   |(34.03143, -118.22981)|\n",
            "|3680 N BUENA PARK DR     |15014 10000 04931|Issued        |07/18/2016     |Insulation           |Approved         |(34.13745, -118.38853)|\n",
            "|1001 N LINDENWOOD LANE   |16042 90000 14712|Permit Finaled|07/20/2016     |Final                |Permit Finaled   |(34.07732, -118.48578)|\n",
            "|2836 S ANCHOR AVE        |15016 20001 17211|CofO Issued   |07/18/2016     |Inspection           |Permit Finaled   |(34.03878, -118.39963)|\n",
            "|2836 S ANCHOR AVE        |15016 20001 17211|CofO Issued   |07/18/2016     |Inspection           |Permit Finaled   |(34.03878, -118.39963)|\n",
            "|5489 E KEATS ST          |16042 10000 12648|Permit Finaled|07/18/2016     |Final                |Permit Finaled   |(34.09126, -118.16077)|\n",
            "|4125 N PERLITA AVE #B    |16016 20000 07926|Issued        |07/18/2016     |Drywall Nailing      |Approved         |(34.12805, -118.26667)|\n",
            "|5744 W MANCHESTER AVE    |01020 10000 02808|Issued        |07/22/2016     |Plumbing Verification|Insp Scheduled   |(33.95971, -118.38187)|\n",
            "|5924-5926 N FIGUEROA ST  |16042 10000 11285|Issued        |07/20/2016     |Rough                |Partial Approval |(34.11074, -118.19008)|\n",
            "+-------------------------+-----------------+--------------+---------------+---------------------+-----------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ins.printSchema()"
      ],
      "metadata": {
        "id": "uC1359-aFY3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b544af-6884-4e9f-f5b7-0f9288a72884"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ADDRESS: string (nullable = true)\n",
            " |-- PERMIT: string (nullable = true)\n",
            " |-- Permit Status: string (nullable = true)\n",
            " |-- Inspection Date: string (nullable = true)\n",
            " |-- Inspection Type: string (nullable = true)\n",
            " |-- Inspection Result: string (nullable = true)\n",
            " |-- Latitude/Longitude: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ins.select([count(when(col(c).isNull(), c)).alias(c) for c in ins.columns]).show()"
      ],
      "metadata": {
        "id": "STu9Zc0lOQmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7acb8be1-5c16-460b-9447-2a910f6c9c30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------------+---------------+---------------+-----------------+------------------+\n",
            "|ADDRESS|PERMIT|Permit Status|Inspection Date|Inspection Type|Inspection Result|Latitude/Longitude|\n",
            "+-------+------+-------------+---------------+---------------+-----------------+------------------+\n",
            "|      1|     0|       481120|         205257|         481137|           481419|            481120|\n",
            "+-------+------+-------------+---------------+---------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "from pyspark.sql.functions import year, month\n",
        "ins = ins.filter(ins[\"Latitude/Longitude\"].isNotNull())\n",
        "ins = ins.withColumn(\"latitude\", regexp_extract(\"Latitude/Longitude\", r\"\\((-?\\d+\\.\\d+),\", 1).cast(\"float\"))\n",
        "ins = ins.withColumn(\"longitude\", regexp_extract(\"Latitude/Longitude\", r\", (-?\\d+\\.\\d+)\\)\", 1).cast(\"float\"))\n",
        "#drop the column called Latitude/Longitude from ins\n",
        "ins = ins.drop(\"Latitude/Longitude\")\n",
        "ins = ins.withColumn(\"Inspection Date\",\n",
        "    when(\n",
        "        ins[\"Inspection Date\"].rlike(r\"\\d{2}/\\d{2}/\\d{4}\"),\n",
        "        to_timestamp(ins[\"Inspection Date\"], \"MM/dd/yyyy\")\n",
        "    ).otherwise(None))\n",
        "\n",
        "\n",
        "# extract the year and month info from inspection date\n",
        "ins = ins.withColumn(\"year\", year(ins[\"Inspection Date\"]))\n",
        "ins = ins.withColumn(\"month\", month(ins[\"Inspection Date\"]))"
      ],
      "metadata": {
        "id": "-794bbCA_MpU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save file as parquet, I used the cell below to save csv during production\n",
        "ins.write.parquet(\"/content/drive/MyDrive/405 final/ins_processed.parquet\")\n",
        "crime.write.parquet(\"/content/drive/MyDrive/405 final/crime_processed.parquet\")"
      ],
      "metadata": {
        "id": "QocPHt3Bqnrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I used this one: save file as 1 single csv\n",
        "ins.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/405 final/ins_csv\")\n",
        "crime.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/drive/MyDrive/405 final/crime_csv\")"
      ],
      "metadata": {
        "id": "rmRJ65tDc8CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCjas6rCF5q8",
        "outputId": "12232d1b-7db8-4cd0-cc21-20c3cc679e40"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-------------------+--------+----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+-----------+------+------------+--------+--------+--------+--------+--------------------+------------+-------+---------+-------------+\n",
            "|    DR_NO|          Date Rptd|           DATE OCC|TIME OCC|AREA|AREA NAME|Rpt Dist No|Part 1-2|Crm Cd|         Crm Cd Desc|       Mocodes|Vict Age|Vict Sex|Vict Descent|Premis Cd|         Premis Desc|Weapon Used Cd|Weapon Desc|Status| Status Desc|Crm Cd 1|Crm Cd 2|Crm Cd 3|Crm Cd 4|            LOCATION|Cross Street|    LAT|      LON|DayOfWeek OCC|\n",
            "+---------+-------------------+-------------------+--------+----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+-----------+------+------------+--------+--------+--------+--------+--------------------+------------+-------+---------+-------------+\n",
            "|190326475|2020-03-01 00:00:00|2020-03-01 00:00:00|      21|   7| Wilshire|        784|       1|   510|    VEHICLE - STOLEN|          NULL|       0|       M|           O|      101|              STREET|          NULL|       NULL|    AA|Adult Arrest|     510|     998|    NULL|    NULL|1900 S  LONGWOOD ...|        NULL|34.0375|-118.3506|       Sunday|\n",
            "|200106753|2020-02-09 00:00:00|2020-02-08 00:00:00|      18|   1|  Central|        182|       1|   330|BURGLARY FROM VEH...|1822 1402 0344|      47|       M|           O|      128|BUS STOP/LAYOVER ...|          NULL|       NULL|    IC| Invest Cont|     330|     998|    NULL|    NULL|1000 S  FLOWER   ...|        NULL|34.0444|-118.2628|     Saturday|\n",
            "|200320258|2020-11-11 00:00:00|2020-11-04 00:00:00|      17|   3|Southwest|        356|       1|   480|       BIKE - STOLEN|     0344 1251|      19|       X|           X|      502|MULTI-UNIT DWELLI...|          NULL|       NULL|    IC| Invest Cont|     480|    NULL|    NULL|    NULL|1400 W  37TH     ...|        NULL| 34.021|-118.3002|    Wednesday|\n",
            "+---------+-------------------+-------------------+--------+----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+-----------+------+------------+--------+--------+--------+--------+--------------------+------------+-------+---------+-------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ins.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3iYYDfMGGHy",
        "outputId": "d71142c6-fa76-4d58-9bd5-564fcf726331"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+--------------+-------------------+-----------------+-----------------+--------+----------+----+-----+\n",
            "|             ADDRESS|           PERMIT| Permit Status|    Inspection Date|  Inspection Type|Inspection Result|latitude| longitude|year|month|\n",
            "+--------------------+-----------------+--------------+-------------------+-----------------+-----------------+--------+----------+----+-----+\n",
            "|10000 W SANTA MON...|14044 10000 02293|        Issued|2016-07-20 00:00:00|Rough-Ventilation| Partial Approval|34.06364|-118.41437|2016|    7|\n",
            "| 1000 S SANTA FE AVE|15016 10000 18196|Permit Finaled|2016-07-22 00:00:00|  Smoke Detectors|   Insp Cancelled|34.03143|-118.22981|2016|    7|\n",
            "|3680 N BUENA PARK DR|15014 10000 04931|        Issued|2016-07-18 00:00:00|       Insulation|         Approved|34.13745|-118.38853|2016|    7|\n",
            "+--------------------+-----------------+--------------+-------------------+-----------------+-----------------+--------+----------+----+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacer: below cells are experimental code that are not used in final scipt"
      ],
      "metadata": {
        "id": "TsCCOa9I4sIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from sedona.register import SedonaRegistrator\n",
        "from sedona.utils.adapter import Adapter\n",
        "from pyspark.sql.functions import col, expr, count\n",
        "\n",
        "SedonaRegistrator.registerAll(spark)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5hMNG9Xq8L6",
        "outputId": "33bcda13-b00d-4241-ab47-c279e80e94d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ae3e9f71d474>:6: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
            "  SedonaRegistrator.registerAll(spark)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
      ],
      "metadata": {
        "id": "nbCJ7FWysidd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ins = ins.withColumn(\"ins_point\", expr(\"ST_Point(longitude, latitude)\"))\n",
        "\n",
        "# Convert `crime` (crime locations) to a spatial DataFrame\n",
        "crime = crime.withColumn(\"crime_point\", expr(\"ST_Point(Lon, Lat)\"))\n",
        "\n",
        "# Create a 200m buffer around each crime location and ensure it is properly formatted\n",
        "crime = crime.withColumn(\"crime_buffer\", expr(\"ST_AsText(ST_Buffer(crime_point, 200))\"))\n",
        "\n",
        "# Convert buffer back to a geometry column for spatial join\n",
        "crime = crime.withColumn(\"crime_buffer\", expr(\"ST_GeomFromWKT(crime_buffer)\"))\n",
        "\n",
        "# Perform spatial join: Find inspections inside crime buffers\n",
        "joined_df = crime.alias(\"c\").join(\n",
        "    ins.alias(\"i\"),\n",
        "    expr(\"ST_Contains(c.crime_buffer, i.ins_point)\"),  # Check if inspection is within buffer\n",
        "    \"left\"\n",
        ").groupby(\"c.Lon\", \"c.Lat\").agg(count(\"i.ins_point\").alias(\"inspection_count\"))\n",
        "\n",
        "\n",
        "# Show results\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "qcBaS8OBsQRr",
        "outputId": "ad82fb3c-5f51-4f96-a9c1-2d3cfecec723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o239.showString.\n: org.apache.spark.SparkException: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.notEnoughMemoryToBuildAndBroadcastTableError(QueryExecutionErrors.scala:2213)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:187)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f550f26f653b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Show results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mjoined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o239.showString.\n: org.apache.spark.SparkException: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.notEnoughMemoryToBuildAndBroadcastTableError(QueryExecutionErrors.scala:2213)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:187)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, ArrayType, StructType, StructField, DoubleType, StringType\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "import json\n",
        "\n",
        "# Haversine distance calculation UDF\n",
        "@F.udf(returnType=IntegerType())\n",
        "def haversine_distance(lon1, lat1, lon2, lat2):\n",
        "    \"\"\"Calculate distance between two points in meters\"\"\"\n",
        "    # Convert decimal degrees to radians\n",
        "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    # Haversine formula\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a))\n",
        "    r = 6371000  # Radius of earth in meters\n",
        "    return int(c * r)\n",
        "\n",
        "# Create grid IDs with slightly higher precision for better performance\n",
        "def get_grid_id(lat, lon, precision=0.01):\n",
        "    \"\"\"Get grid cell id for approximate location bucketing\"\"\"\n",
        "    return f\"{int(lat/precision)},{int(lon/precision)}\"\n",
        "\n",
        "# Add grid IDs to both dataframes\n",
        "crime = crime.withColumn(\"grid_id\",\n",
        "    F.udf(get_grid_id, StringType())(F.col(\"latitude\"), F.col(\"longitude\")))\n",
        "\n",
        "ins = ins.withColumn(\"grid_id\",\n",
        "    F.udf(get_grid_id, StringType())(F.col(\"latitude\"), F.col(\"longitude\")))\n",
        "\n",
        "# Create a map of grid cells to inspection points\n",
        "# This will be small enough to broadcast\n",
        "ins_by_grid = ins.groupBy(\"grid_id\").agg(\n",
        "    F.collect_list(\n",
        "        F.struct(\"longitude\", \"latitude\")\n",
        "    ).alias(\"inspection_points\")\n",
        ")\n",
        "\n",
        "# Convert to a Python dictionary for broadcasting\n",
        "ins_grid_dict = {row[\"grid_id\"]: row[\"inspection_points\"]\n",
        "                 for row in ins_by_grid.collect()}\n",
        "\n",
        "# Broadcast the dictionary to all executors\n",
        "ins_grid_broadcast = spark.sparkContext.broadcast(ins_grid_dict)\n",
        "\n",
        "# Create a UDF that counts nearby inspections using the broadcast variable\n",
        "@F.udf(returnType=IntegerType())\n",
        "def count_nearby_inspections(grid_id, crime_lon, crime_lat):\n",
        "    \"\"\"Count inspections within 500m of a crime location\"\"\"\n",
        "    # Get inspection points in the same grid cell\n",
        "    inspection_points = ins_grid_broadcast.value.get(grid_id, [])\n",
        "\n",
        "    # Count points within 500m\n",
        "    count = 0\n",
        "    for point in inspection_points:\n",
        "        ins_lon = point[\"longitude\"]\n",
        "        ins_lat = point[\"latitude\"]\n",
        "\n",
        "        # Convert to radians\n",
        "        lon1, lat1, lon2, lat2 = map(radians, [crime_lon, crime_lat, ins_lon, ins_lat])\n",
        "\n",
        "        # Haversine formula\n",
        "        dlon = lon2 - lon1\n",
        "        dlat = lat2 - lat1\n",
        "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "        c = 2 * asin(sqrt(a))\n",
        "        r = 6371000  # Radius of earth in meters\n",
        "        distance = c * r\n",
        "\n",
        "        if distance <= 500:\n",
        "            count += 1\n",
        "\n",
        "    return count\n",
        "\n",
        "# Apply the UDF to count nearby inspections for each crime\n",
        "result = crime.withColumn(\n",
        "    \"nearby_inspections_count\",\n",
        "    count_nearby_inspections(\n",
        "        F.col(\"grid_id\"),\n",
        "        F.col(\"longitude\"),\n",
        "        F.col(\"latitude\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display result\n",
        "result.select(\"latitude\", \"longitude\", \"nearby_inspections_count\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "2sIxHteM-oGK",
        "outputId": "46993fe1-963d-403a-8389-673c4ee58089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8c13094fb405>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Display result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LAT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nearby_inspections_count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}